import json
import subprocess
import os
import time
import re
import requests
import urllib3
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==========================================
# âš™ï¸ ç³»ç»Ÿæ ¸å¿ƒé…ç½® (System Configuration)
# ==========================================
TARGET_FILES = ['TV.m3u8', 'no sex/TV_1(no sex).m3u8']
JSON_FILE = 'streams.json'

# å±è”½ requests è¯·æ±‚ verify=False æ—¶çš„çƒ¦äººè­¦å‘Šï¼Œä¿æŒç•Œé¢æ¸…çˆ½
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

UA_LIST = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
    'Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1'
]
BATCH_SIZE = 10     # å¹¶å‘å¤„ç†é˜ˆå€¼
COOKIE_TEMP_FILE = 'cookies_netscape.txt' # ä»…ä½œä¸ºè¿è¡Œæ—¶ä¸´æ—¶æ–‡ä»¶ï¼Œä¸ä¸Šä¼ 

def get_random_ua():
    import random
    return random.choice(UA_LIST)

# ==========================================
# ğŸ” é‰´æƒå‡­è¯å¤„ç†å­ç³»ç»Ÿ (Credential Subsystem)
# ==========================================
def process_smart_cookies():
    """
    [é‰´æƒé€»è¾‘] ä¼˜å…ˆä»äº‘ç«¯ç¯å¢ƒå˜é‡åŠ è½½ï¼Œé¿å…æœ¬åœ°æ–‡ä»¶ä¾èµ–
    """
    content = None

    if 'YOUTUBE_COOKIES' in os.environ and os.environ['YOUTUBE_COOKIES'].strip():
        print("    [é‰´æƒä¸­å¿ƒ] â˜ï¸ æ£€æµ‹åˆ°äº‘ç«¯ç¯å¢ƒå˜é‡å¯†é’¥ï¼Œæ­£åœ¨åŠ è½½...")
        content = os.environ['YOUTUBE_COOKIES']
    elif os.path.exists('cookies.txt'):
        try:
            with open('cookies.txt', 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read().strip()
            if content:
                print("    [é‰´æƒä¸­å¿ƒ] ğŸ“‚ æ£€æµ‹åˆ°æœ¬åœ°å‡­è¯æ–‡ä»¶ï¼Œæ­£åœ¨åŠ è½½...")
        except: pass

    if not content: 
        print("    [é‰´æƒä¸­å¿ƒ] âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆå‡­è¯ï¼Œå°†ä»¥è®¿å®¢æ¨¡å¼è¿è¡Œã€‚")
        return False

    try:
        if content.startswith('[') or content.startswith('{'):
            try:
                data = json.loads(content)
                if isinstance(data, dict): data = [data]
                with open(COOKIE_TEMP_FILE, 'w', encoding='utf-8') as out:
                    out.write("# Netscape HTTP Cookie File\n")
                    for c in data:
                        if 'domain' not in c or 'name' not in c: continue
                        domain = c.get('domain', '')
                        if not domain.startswith('.'): domain = '.' + domain
                        expiry = str(int(c.get('expirationDate', time.time() + 31536000)))
                        out.write(f"{domain}\tTRUE\t{c.get('path','/')}\tTRUE\t{expiry}\t{c.get('name')}\t{c.get('value')}\n")
                print(f"    [é‰´æƒä¸­å¿ƒ] âœ… JSON æ ¼å¼å‡­è¯è½¬æ¢å®Œæ¯•")
                return True
            except:
                print(f"    [é‰´æƒä¸­å¿ƒ] âš ï¸ JSON è§£æå¼‚å¸¸ï¼Œå°è¯•åˆ‡æ¢è‡³å…¼å®¹æ¨¡å¼...")

        if "# Netscape" in content or content.count('\t') > 3:
            with open(COOKIE_TEMP_FILE, 'w', encoding='utf-8') as out:
                out.write(content)
            print(f"    [é‰´æƒä¸­å¿ƒ] âœ… æ ‡å‡† Netscape æ ¼å¼åŠ è½½å®Œæ¯•")
            return True

        print("    [é‰´æƒä¸­å¿ƒ] âš ï¸ æ ¼å¼æœªè¯†åˆ«ï¼Œå¯ç”¨å¯å‘å¼å…¼å®¹æ¨¡å¼...")
        with open(COOKIE_TEMP_FILE, 'w', encoding='utf-8') as out:
            out.write("# Netscape HTTP Cookie File\n")
            expiry = str(int(time.time() + 31536000))
            for pair in content.split(';'):
                if '=' in pair:
                    try:
                        name, value = pair.strip().split('=', 1)
                        out.write(f".youtube.com\tTRUE\t/\tTRUE\t{expiry}\t{name}\t{value}\n")
                    except: continue
        print(f"    [é‰´æƒä¸­å¿ƒ] âœ… å…¼å®¹æ€§è½¬æ¢å®Œæˆ")
        return True

    except Exception as e:
        print(f"    [é‰´æƒä¸­å¿ƒ] âŒ å‡­è¯å¤„ç†æµç¨‹è‡´å‘½é”™è¯¯: {e}")
        return False

# --- æ·±åº¦ç½‘é¡µå—…æ¢å™¨ (Web Sniffer - Requestsç‰ˆ) ---
def sniff_m3u8_from_web(url, ua):
    """
    [å…œåº•é€»è¾‘] æ¨¡æ‹Ÿ Web Video Casterï¼Œä½¿ç”¨ requests é«˜çº§åº“è¿›è¡Œå—…æ¢
    """
    try:
        headers = {
            'User-Agent': ua,
            'Referer': url,  # å¾ˆå¤šç½‘ç«™æ£€æŸ¥ Referer
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        }
        # verify=False å¿½ç•¥è¯ä¹¦é”™è¯¯ï¼Œallow_redirects=True å…è®¸è·³è½¬
        response = requests.get(url, headers=headers, timeout=15, verify=False, allow_redirects=True)
        response.encoding = response.apparent_encoding # è‡ªåŠ¨è¯†åˆ«ç¼–ç ï¼Œé˜²æ­¢ä¹±ç 
        
        html = response.text
        
        # æ­£åˆ™åŒ¹é…ï¼šå¯»æ‰¾ http/https å¼€å¤´ï¼Œ.m3u8 ç»“å°¾çš„å­—ç¬¦ä¸²
        # å…¼å®¹è½¬ä¹‰å­—ç¬¦ (ä¾‹å¦‚ \/ -> /)
        pattern = r'(http[s]?:\\?/\\?/[^\s"\'<>]+?\.m3u8[^\s"\'<>]*)'
        matches = re.findall(pattern, html)
        
        if matches:
            # å–ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹ï¼Œå¹¶å¤„ç†è½¬ä¹‰æ–œæ 
            found_url = matches[0].replace('\\/', '/')
            return found_url
    except Exception:
        # requests çš„å¼‚å¸¸æˆ‘ä»¬é™é»˜å¤„ç†
        pass
    return None

# --- æ ¸å¿ƒè§£ææ¨¡å— (æ··åˆå¼•æ“ç‰ˆ) ---
def get_real_url(url, channel_name, retry_mode=False):
    is_yt = 'youtube.com' in url or 'youtu.be' in url
    current_ua = get_random_ua()
    
    # -------------------------------
    # ç­–ç•¥ A: yt-dlp æ ‡å‡†è§£æ
    # -------------------------------
    cmd = ['yt-dlp', '-g', '--no-playlist', '--no-check-certificate', '--user-agent', current_ua]
    
    # é’ˆå¯¹ YouTube å¼ºåˆ¶ HLSï¼Œé’ˆå¯¹é€šç”¨ç½‘ç«™ä¼˜å…ˆ HLS å…è®¸å›é€€
    if is_yt:
        cmd.extend(['-f', 'best[protocol^=m3u8]/best'])
        cmd.extend(['--referer', 'https://www.youtube.com/'])
        if os.path.exists(COOKIE_TEMP_FILE): cmd.extend(['--cookies', COOKIE_TEMP_FILE])
    else:
        # éæ²¹ç®¡ï¼šå°è¯•é€šç”¨æå–å™¨ï¼Œç§»é™¤ mp4 é™åˆ¶ï¼Œè®©å®ƒè‡ªå·±æ‰¾
        cmd.extend(['--referer', url])
    
    cmd.append(url)
    
    try:
        res = subprocess.run(cmd, capture_output=True, text=True, timeout=45)
        if res.returncode == 0:
            raw_output = res.stdout.strip()
            real_url = raw_output.split('\n')[0] if raw_output else None
            if real_url and 'http' in real_url:
                return channel_name, real_url, True
    except:
        pass

    # -------------------------------
    # ç­–ç•¥ B: æ·±åº¦æŒ–æ˜ (ä»…é YouTube è§¦å‘)
    # -------------------------------
    if not is_yt:
        # å¦‚æœ yt-dlp å¤±è´¥ï¼Œå°è¯•æ¨¡æ‹Ÿæµè§ˆå™¨å—…æ¢ M3U8 (è°ƒç”¨ requests ç‰ˆ)
        sniffed_url = sniff_m3u8_from_web(url, current_ua)
        if sniffed_url:
            return channel_name, sniffed_url, True

    return channel_name, None, False

# --- ä¸»ç¨‹åºå…¥å£ ---
def update_streams():
    if not os.path.exists(JSON_FILE): return

    # 1. æ‰§è¡Œé‰´æƒ
    process_smart_cookies()
    
    # å®¹é”™è¯»å– JSON
    try:
        with open(JSON_FILE, 'r', encoding='utf-8') as f: data = json.load(f)
    except Exception as e:
        print(f"âŒ JSON é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        return

    if "Run_Series_Loop" in data:
        data.pop("Run_Series_Loop") 
    
    stream_map = {}
    def extract(d):
        for k, v in d.items():
            if isinstance(v, dict): extract(v)
            elif isinstance(v, str) and v.startswith(('http', 'rtmp')): stream_map[k] = v
    extract(data)

    unique_tasks = {}
    for m in TARGET_FILES:
        if os.path.exists(m):
            with open(m, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.startswith('#EXTINF:'):
                        name = line.split(',')[-1].strip()
                        if name in stream_map: unique_tasks[name] = stream_map[name]

    live_tasks = [(k, v) for k, v in unique_tasks.items()]
    failed_channels = []
    
    print(f">>> [ä»»åŠ¡å°±ç»ª] ç›´æ’­é˜Ÿåˆ—: {len(live_tasks)}")

    # ==========================================
    # Phase 1: é«˜ä¼˜å…ˆçº§ - ç›´æ’­é¢‘é“ (Live Channels)
    # ==========================================
    if live_tasks:
        print(f"\n========================================")
        print(f"ğŸš€ [ç¬¬ä¸€é˜¶æ®µ] æ­£åœ¨æ›´æ–°ç›´æ’­é¢‘é“...")
        print(f"========================================")
        
        for i in range(0, len(live_tasks), BATCH_SIZE):
            batch = live_tasks[i:i+BATCH_SIZE]
            print(f"\nâš¡ [æ‰¹æ¬¡æ‰§è¡Œ] åºåˆ—: {i//BATCH_SIZE + 1}...")
            
            with ThreadPoolExecutor(max_workers=BATCH_SIZE) as executor:
                futures = {executor.submit(get_real_url, u, n, False): n for n, u in batch}
                for future in as_completed(futures):
                    n, u, success = future.result()
                    if success and u:
                        # æ£€æµ‹æ˜¯å¦æ˜¯é€šè¿‡å—…æ¢è·å–çš„é“¾æ¥ (ç®€å•çš„é€»è¾‘åˆ¤æ–­)
                        is_sniffed = '.m3u8' in u and 'googlevideo' not in u and 'bilivideo' not in u
                        tag = "ğŸ” [ç½‘é¡µå—…æ¢]" if is_sniffed else "âœ… [è§£ææˆåŠŸ]"
                        print(f"   {tag} {n}") 
                        unique_tasks[n] = u
                    else:
                        print(f"   ğŸŒªï¸ [æš‚ç¼“å¤„ç†] {n}")
                        orig = next((url for name, url in batch if name == n), None)
                        if orig: failed_channels.append((n, orig))
            time.sleep(0.5)

    # ==========================================
    # Phase 2: æœ€ç»ˆæŒ½æ•‘ - å…¨å±€é‡è¯• (Global Retry)
    # ==========================================
    if failed_channels:
        print(f"\n========================================")
        print(f"ğŸ”„ [æœ€ç»ˆæŒ½æ•‘] é›†ä¸­å¤„ç†æ‰€æœ‰å¼‚å¸¸ä»»åŠ¡...")
        print(f"========================================")
        
        print(f"   >>> æ­£åœ¨ä¿®å¤ {len(failed_channels)} ä¸ªç›´æ’­ä¿¡å·...")
        for idx, (n, u) in enumerate(failed_channels):
            print(f"   ğŸ› ï¸ [æ­£åœ¨ä¿®å¤] {n} ...")
            retry_success = False
            for r_attempt in range(1, 3):
                _, new_u, success = get_real_url(u, n, True)
                if success and new_u:
                    print(f"      âœ… [å›æ»šæˆåŠŸ] é“¾è·¯å·²æ¢å¤")
                    unique_tasks[n] = new_u
                    retry_success = True
                    break
                else:
                    time.sleep(2)
            if not retry_success: print(f"      âŒ [æœ€ç»ˆç†”æ–­] æ— æ³•æ¥é€šï¼Œå·²å¼ƒç”¨")

    # ==========================================
    # I/O æŒä¹…åŒ– (Persistence)
    # ==========================================
    
    print("\n>>> [I/O æ“ä½œ] æ­£åœ¨å†™å…¥ç›®æ ‡æ–‡ä»¶...")
    for m in TARGET_FILES:
        if not os.path.exists(m): continue
        
        with open(m, 'r', encoding='utf-8') as f: lines = f.readlines()
        new_lines, idx, cnt = [], 0, 0
        while idx < len(lines):
            line = lines[idx]
            if line.startswith('#EXTINF:'):
                name = line.split(',')[-1].strip()
                if name in unique_tasks:
                    new_lines.append(line)
                    new_lines.append(unique_tasks[name] + '\n')
                    idx += 2; cnt += 1; continue
            new_lines.append(line); idx += 1
        with open(m, 'w', encoding='utf-8') as f: f.writelines(new_lines)
        print(f"   -> {m}: æ›´æ–°è®°å½• {cnt} æ¡")
    
    print("\nâœ… [æ‰§è¡Œå®Œæ¯•] æ‰€æœ‰è®¡åˆ’ä»»åŠ¡å·²å®Œæˆã€‚")

if __name__ == '__main__':
    update_streams()
