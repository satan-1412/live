import json
import subprocess
import os
import time
import re
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==========================================
# âš™ï¸ ç³»ç»Ÿæ ¸å¿ƒé…ç½® (System Configuration)
# ==========================================
TARGET_FILES = ['TV.m3u8', 'no sex/TV_1(no sex).m3u8']
JSON_FILE = 'streams.json'
# æ¯æ¬¡è¯·æ±‚çš„è¶…æ—¶æ—¶é—´ (ç§’)
TIMEOUT = 15 

UA_LIST = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
    'Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1'
]
BATCH_SIZE = 8     # å¹¶å‘æ•°
COOKIE_TEMP_FILE = 'cookies_netscape.txt'

def get_random_ua():
    import random
    return random.choice(UA_LIST)

# ==========================================
# ğŸ•µï¸â€â™‚ï¸ å—…æ¢ä¸“å®¶ (Smart Sniffer)
# ==========================================
def smart_sniffer(url):
    """
    å½“ yt-dlp å¤±è´¥æ—¶ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨å»ç½‘é¡µæºä»£ç é‡Œâ€œæ‰’â€ m3u8 é“¾æ¥
    """
    print(f"      ğŸ” [å—…æ¢æ¨¡å¼] æ­£åœ¨æ‰«æç½‘é¡µæºç : {url} ...")
    
    headers = {
        'User-Agent': get_random_ua(),
        'Referer': url,  # å¾ˆå¤šç½‘ç«™éœ€è¦ Referer æ‰èƒ½è®¿é—® m3u8
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
    }

    try:
        # 1. è·å–ç½‘é¡µæºç 
        resp = requests.get(url, headers=headers, timeout=TIMEOUT, verify=False)
        resp.encoding = 'utf-8' # å¼ºåˆ¶ UTF-8ï¼Œé˜²æ­¢ä¹±ç 
        html_content = resp.text

        # 2. æ­£åˆ™æš´åŠ›åŒ¹é… (åŒ¹é… http/https å¼€å¤´ï¼Œ.m3u8 ç»“å°¾ï¼Œä¸­é—´å…è®¸å¸¦å‚æ•°)
        # è§£é‡Š: ["'] æ˜¯åŒ¹é…å¼•å·å¼€å¤´, (https?://[^"']+\.m3u8[^"']*) æ˜¯æ•è·ç»„
        pattern = r'["\'](https?://[^"\']+\.m3u8[^"\']*)["\']'
        matches = re.findall(pattern, html_content)

        if matches:
            # å»é‡å¹¶å–ç¬¬ä¸€ä¸ªçœ‹èµ·æ¥æœ€çŸ­çš„ï¼ˆé€šå¸¸é•¿é“¾æ¥å¸¦äº†å¤ªå¤šæ— ç”¨tokenï¼Œå®¹æ˜“å¤±æ•ˆï¼Œæˆ–è€…å–ç¬¬ä¸€ä¸ªå‘ç°çš„ï¼‰
            # è¿™é‡Œç®€å•ç­–ç•¥ï¼šå–ç¬¬ä¸€ä¸ª
            found_url = matches[0]
            
            # å¤„ç†ä¸€ä¸‹å¯èƒ½çš„è½¬ä¹‰å­—ç¬¦ (å¦‚ \/)
            found_url = found_url.replace('\\/', '/')
            
            print(f"      ğŸ‰ [å—…æ¢æˆåŠŸ] æ•è·ä¿¡å·: {found_url[:60]}...")
            return found_url
        
        # 3. å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æ‰¾ iframe (ç®€å•çš„ iframe ç©¿é€)
        iframe_pattern = r'<iframe[^>]+src=["\'](https?://[^"\']+)["\']'
        iframes = re.findall(iframe_pattern, html_content)
        for iframe_url in iframes:
            # æ’é™¤å¸¸è§å¹¿å‘Š iframe
            if "google" in iframe_url or "facebook" in iframe_url: continue
            
            print(f"      ğŸ•³ï¸ [æ·±åº¦é’»å–] å‘ç° iframeï¼Œæ­£åœ¨æ½œå…¥: {iframe_url[:40]}...")
            try:
                sub_resp = requests.get(iframe_url, headers=headers, timeout=TIMEOUT, verify=False)
                sub_matches = re.findall(pattern, sub_resp.text)
                if sub_matches:
                    found_url = sub_matches[0].replace('\\/', '/')
                    print(f"      ğŸ‰ [å—…æ¢æˆåŠŸ] åœ¨ iframe ä¸­æ•è·: {found_url[:60]}...")
                    return found_url
            except:
                pass

    except Exception as e:
        print(f"      âŒ [å—…æ¢å¤±è´¥] {e}")

    return None

# ==========================================
# ğŸ” é‰´æƒå‡­è¯å¤„ç† (ä¿æŒåŸæ ·)
# ==========================================
def process_smart_cookies():
    # ... (ä¿æŒåŸæ¥çš„é€»è¾‘ä¸å˜ï¼Œä¸ºäº†èŠ‚çœç¯‡å¹…è¿™é‡Œçœç•¥ï¼Œè¯·ä¿ç•™åŸæ¥çš„é‰´æƒä»£ç ) ...
    # å¦‚æœä½ è¿™éƒ¨åˆ†æ²¡æ”¹åŠ¨ï¼Œç›´æ¥å¤åˆ¶åŸæ¥çš„ process_smart_cookies å‡½æ•°å³å¯
    pass 

# ä¸ºäº†ç¡®ä¿ä»£ç å®Œæ•´è¿è¡Œï¼Œè¿™é‡Œè¿˜æ˜¯è¡¥ä¸Šä¸€ä¸ªç®€åŒ–çš„é‰´æƒæ£€æµ‹ï¼Œå®é™…ä½¿ç”¨è¯·ç”¨ä½ åŸæ¥çš„å®Œæ•´ç‰ˆ
if not os.path.exists(COOKIE_TEMP_FILE) and os.path.exists('cookies.txt'):
    # ç®€å•è½¬æ¢ä¸€ä¸‹ï¼Œé˜²æ­¢æŠ¥é”™
    try:
        with open('cookies.txt', 'r') as f, open(COOKIE_TEMP_FILE, 'w') as o:
            o.write(f.read())
    except: pass

# ==========================================
# ğŸ“¡ æ ¸å¿ƒè§£ææ¨¡å— (Core Resolver)
# ==========================================
def get_real_url(url, channel_name, retry_mode=False):
    # --- ç­–ç•¥ 1: ç›´é“¾é€ä¼  (Pass-through) ---
    # å¦‚æœç”¨æˆ·å¡«çš„æœ¬æ¥å°±æ˜¯ .m3u8 æˆ– .mp4ï¼Œç›´æ¥æ£€æµ‹æ˜¯å¦å­˜æ´»ï¼Œä¸èµ° yt-dlp
    if '.m3u8' in url or '.mp4' in url or '.flv' in url:
        return channel_name, url, True

    # å®šä¹‰ yt-dlp å‘½ä»¤
    is_yt = 'youtube.com' in url or 'youtu.be' in url
    cmd = ['yt-dlp', '-g', '--no-playlist', '--no-check-certificate', '--user-agent', get_random_ua()]
    
    if is_yt:
        cmd.extend(['-f', 'best[protocol^=m3u8]/best'])
        cmd.extend(['--referer', 'https://www.youtube.com/'])
        if os.path.exists(COOKIE_TEMP_FILE): cmd.extend(['--cookies', COOKIE_TEMP_FILE])     
    else:
        # å¯¹äºé YouTube ç½‘ç«™ï¼Œæ”¾å®½æ ¼å¼é™åˆ¶ï¼Œä¼˜å…ˆæ‰¾ HLS
        cmd.extend(['-f', 'best'])

    cmd.append(url)
    
    # --- ç­–ç•¥ 2: yt-dlp å®˜æ–¹è§£æ (Standard Extraction) ---
    try:
        # ç»™ yt-dlp å¤šä¸€ç‚¹æ—¶é—´ï¼Œæœ‰äº›ç›´æ’­åŠ è½½æ…¢
        res = subprocess.run(cmd, capture_output=True, text=True, timeout=35)
        if res.returncode == 0:
            raw_output = res.stdout.strip()
            real_url = raw_output.split('\n')[0] if raw_output else None
            if real_url and 'http' in real_url:
                return channel_name, real_url, True
        else:
            # å¦‚æœæ˜¯ YouTube ä¸”å¤±è´¥äº†ï¼Œé€šå¸¸æ²¡æ•‘äº†ï¼Œä¸ç”¨èµ°å—…æ¢
            if is_yt:
                # print(f"   âš ï¸ [YT-DLP é”™è¯¯] {res.stderr[:50]}...") # è°ƒè¯•ç”¨
                return channel_name, None, False
    except Exception as e:
        pass

    # --- ç­–ç•¥ 3: ç½‘é¡µå—…æ¢ (Web Sniffer) ---
    # å¦‚æœä¸æ˜¯ YouTubeï¼Œä¸” yt-dlp å¤±è´¥äº†ï¼Œå¯ç”¨å—…æ¢å™¨
    if not is_yt:
        sniffed_url = smart_sniffer(url)
        if sniffed_url:
            return channel_name, sniffed_url, True

    return channel_name, None, False

# ==========================================
# ğŸ”„ ä¸»ç¨‹åº (Main Loop)
# ==========================================
def update_streams():
    # å¿½ç•¥ SSL è­¦å‘Š
    requests.packages.urllib3.disable_warnings()

    if not os.path.exists(JSON_FILE): return

    # 1. é‰´æƒ (è¿™é‡Œè°ƒç”¨ä½ åŸæ¥çš„é‰´æƒé€»è¾‘ï¼Œæˆ–è€…ä¸Šé¢ç®€åŒ–çš„)
    # process_smart_cookies() 
    
    try:
        with open(JSON_FILE, 'r', encoding='utf-8') as f: data = json.load(f)
    except Exception as e:
        print(f"âŒ JSON é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        return

    if "Run_Series_Loop" in data: data.pop("Run_Series_Loop") 
    
    stream_map = {}
    def extract(d):
        for k, v in d.items():
            if isinstance(v, dict): extract(v)
            elif isinstance(v, str) and v.startswith(('http', 'rtmp')): stream_map[k] = v
    extract(data)

    # ä»…æ›´æ–°å·²ç»åœ¨ m3u8 æ–‡ä»¶é‡Œå­˜åœ¨çš„é¢‘é“
    unique_tasks = {}
    for m in TARGET_FILES:
        if os.path.exists(m):
            with open(m, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.startswith('#EXTINF:'):
                        name = line.split(',')[-1].strip()
                        if name in stream_map: unique_tasks[name] = stream_map[name]

    live_tasks = [(k, v) for k, v in unique_tasks.items()]
    failed_channels = []
    
    print(f">>> [ä»»åŠ¡å°±ç»ª] å¾…æ£€æµ‹é˜Ÿåˆ—: {len(live_tasks)}")

    if live_tasks:
        print(f"\n========================================")
        print(f"ğŸš€ [æ‰§è¡Œä¸­] æ­£åœ¨æ›´æ–°é“¾æ¥ (æ··åˆå¼•æ“)...")
        print(f"========================================")
        
        for i in range(0, len(live_tasks), BATCH_SIZE):
            batch = live_tasks[i:i+BATCH_SIZE]
            print(f"\nâš¡ [æ‰¹æ¬¡ {i//BATCH_SIZE + 1}] Processing...")
            
            with ThreadPoolExecutor(max_workers=BATCH_SIZE) as executor:
                futures = {executor.submit(get_real_url, u, n, False): n for n, u in batch}
                for future in as_completed(futures):
                    n, u, success = future.result()
                    if success and u:
                        print(f"   âœ… [è¿æ¥æˆåŠŸ] {n}")
                        unique_tasks[n] = u
                    else:
                        print(f"   ğŸŒªï¸ [è·å–å¤±è´¥] {n}")
                        # å¤±è´¥ä¸æ›´æ–°ï¼Œä¿ç•™åŸé“¾æ¥ï¼ˆæˆ–è€…è§†æƒ…å†µå¤„ç†ï¼‰
                        # orig = next((url for name, url in batch if name == n), None)
                        # if orig: failed_channels.append((n, orig))

    # ==========================================
    # I/O æŒä¹…åŒ–
    # ==========================================
    print("\n>>> [I/O æ“ä½œ] æ­£åœ¨å†™å…¥æ–‡ä»¶...")
    for m in TARGET_FILES:
        if not os.path.exists(m): continue
        
        try:
            with open(m, 'r', encoding='utf-8') as f: lines = f.readlines()
            new_lines, idx, cnt = [], 0, 0
            while idx < len(lines):
                line = lines[idx]
                if line.startswith('#EXTINF:'):
                    name = line.split(',')[-1].strip()
                    # å¦‚æœè¯¥é¢‘é“åœ¨ä»»åŠ¡åˆ—è¡¨ä¸­ï¼Œä¸”æˆ‘ä»¬ç¡®å®æ‹¿åˆ°äº†æ–°é“¾æ¥ï¼ˆä¸ä¸ºç©ºï¼‰
                    if name in unique_tasks and unique_tasks[name]:
                        new_lines.append(line)
                        new_lines.append(unique_tasks[name] + '\n')
                        idx += 2; cnt += 1; continue
                new_lines.append(line); idx += 1
            
            with open(m, 'w', encoding='utf-8') as f: f.writelines(new_lines)
            print(f"   -> {m}: å·²æ›´æ–° {cnt} ä¸ªé¢‘é“")
        except Exception as e:
            print(f"   âŒ å†™å…¥å‡ºé”™ {m}: {e}")
    
    print("\nâœ… [å®Œæˆ] æ‰€æœ‰æ“ä½œå·²ç»“æŸã€‚")

if __name__ == '__main__':
    update_streams()
