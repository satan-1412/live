import json
import subprocess
import os
import time
import re
import requests
import urllib3
import urllib.parse
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==========================================
# âš™ï¸ ç³»ç»Ÿæ ¸å¿ƒé…ç½®
# ==========================================
TARGET_FILES = ['TV.m3u8', 'no sex/TV_1(no sex).m3u8']
JSON_FILE = 'streams.json'

# å±è”½è¯ä¹¦è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ä¼ªè£…æ± ï¼šä¸“é—¨é’ˆå¯¹ä¸åŒç±»å‹çš„ç½‘ç«™
UA_POOL = {
    # æ¨¡æ‹Ÿå®‰å“æ‰‹æœºï¼ˆæœ€å®¹æ˜“è·å–ç›´é“¾ï¼‰
    "Android": "Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36",
    # æ¨¡æ‹Ÿè‹¹æœæ‰‹æœº
    "iOS": "Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1",
    # æ¨¡æ‹Ÿç”µè„‘
    "PC": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

BATCH_SIZE = 10     
COOKIE_TEMP_FILE = 'cookies_netscape.txt'

# ==========================================
# ğŸ” é‰´æƒå‡­è¯å¤„ç†
# ==========================================
def process_smart_cookies():
    content = None
    if 'YOUTUBE_COOKIES' in os.environ and os.environ['YOUTUBE_COOKIES'].strip():
        content = os.environ['YOUTUBE_COOKIES']
    elif os.path.exists('cookies.txt'):
        try:
            with open('cookies.txt', 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read().strip()
        except: pass

    if not content: return False

    try:
        if content.startswith('[') or content.startswith('{'):
            try:
                data = json.loads(content)
                if isinstance(data, dict): data = [data]
                with open(COOKIE_TEMP_FILE, 'w', encoding='utf-8') as out:
                    out.write("# Netscape HTTP Cookie File\n")
                    for c in data:
                        if 'domain' not in c or 'name' not in c: continue
                        domain = c.get('domain', '')
                        if not domain.startswith('.'): domain = '.' + domain
                        expiry = str(int(c.get('expirationDate', time.time() + 31536000)))
                        out.write(f"{domain}\tTRUE\t{c.get('path','/')}\tTRUE\t{expiry}\t{c.get('name')}\t{c.get('value')}\n")
                return True
            except: pass

        if "# Netscape" in content or content.count('\t') > 3:
            with open(COOKIE_TEMP_FILE, 'w', encoding='utf-8') as out:
                out.write(content)
            return True
        
        with open(COOKIE_TEMP_FILE, 'w', encoding='utf-8') as out:
            out.write("# Netscape HTTP Cookie File\n")
            expiry = str(int(time.time() + 31536000))
            for pair in content.split(';'):
                if '=' in pair:
                    try:
                        name, value = pair.strip().split('=', 1)
                        out.write(f".youtube.com\tTRUE\t/\tTRUE\t{expiry}\t{name}\t{value}\n")
                    except: continue
        return True
    except Exception:
        return False

# ==========================================
# ğŸ•¸ï¸ æ ¸å¼¹çº§ç½‘é¡µå—…æ¢å™¨ (Nuclear Sniffer)
# ==========================================

def clean_and_validate(url):
    """
    æ¸…æ´—å™¨ï¼šæŠŠå„ç§å˜æ€çš„è½¬ä¹‰å­—ç¬¦è¿˜åŸæˆäººç±»èƒ½çœ‹çš„ URL
    ä¾‹å¦‚ï¼šhttp:\/\/ -> http://
    """
    try:
        # 1. å¤„ç† JSON é£æ ¼çš„åæ–œæ è½¬ä¹‰ (http:\/\/...)
        url = url.replace(r'\/', '/')
        # 2. å¤„ç† URL ç¼–ç  (http%3A%2F%2F...)
        if '%' in url:
            url = urllib.parse.unquote(url)
        # 3. å¤„ç† Unicode è½¬ä¹‰ (\u002F)
        if '\\u' in url:
            url = url.encode('utf-8').decode('unicode_escape')
        
        url = url.strip()
        
        # å†æ¬¡æ£€æŸ¥å¤´éƒ¨ï¼Œé˜²æ­¢è§£å‡ºæ¥æ˜¯ // å¼€å¤´çš„ç›¸å¯¹è·¯å¾„
        if url.startswith('//'):
            url = 'https:' + url
            
        # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆé“¾æ¥
        if url.startswith('http') and ('.m3u8' in url or '.flv' in url or 'm3u8?' in url):
            return url
    except:
        pass
    return None

def find_m3u8_deep(text):
    """
    [æ ¸å¿ƒç®—æ³•] æ­£åˆ™æ ¸å¼¹ï¼šä¸æ”¾è¿‡ä»»ä½•ä¸€ä¸ªåƒé“¾æ¥çš„å­—ç¬¦ä¸²
    """
    candidates = []
    
    # âš¡ æ­£åˆ™ 1: æ ‡å‡†æˆ–è½¬ä¹‰çš„ http é“¾æ¥ (æ•æ‰ http:// å’Œ http:\/\/ å’Œ http%3A%2F%2F)
    # è§£é‡Šï¼šhttps? åé¢è·Ÿç€ (å†’å· æˆ– %3A) ç„¶åæ˜¯ (æ–œæ  æˆ– %2F æˆ– åæ–œæ ) é‡å¤ä¸¤æ¬¡
    pattern_universal = r'(https?[:%3A\\]+[\/%2F\\]+[^"\s\'<>{}|\\^`]+?\.m3u8[^"\s\'<>{}|\\^`]*)'
    matches = re.findall(pattern_universal, text, re.I)
    candidates.extend(matches)
    
    # âš¡ æ­£åˆ™ 2: ä¸“é—¨é’ˆå¯¹å±±ä¸œå«è§† iqilu çš„ç‰¹å¾ (tstreamlive)
    # å³ä½¿å®ƒä¸ä»¥ .m3u8 ç»“å°¾ï¼Œåªè¦åŒ…å«è¿™ä¸ªæ ¸å¿ƒåŸŸåä¸”çœ‹èµ·æ¥åƒä¸ªé•¿é“¾æ¥ï¼Œä¹ŸæŠ“å‡ºæ¥çœ‹çœ‹
    if 'iqilu.com' in text or 'tstreamlive' in text:
        pattern_iqilu = r'(https?[:\\]+[\/\\].+?tstreamlive.+?\.m3u8[^"\s\'<>]*)'
        matches_iqilu = re.findall(pattern_iqilu, text, re.I)
        candidates.extend(matches_iqilu)

    # æ¸…æ´—å¹¶è¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨çš„
    for u in candidates:
        clean_url = clean_and_validate(u)
        if clean_url:
            return clean_url
            
    return None

def sniff_single_ua(url, ua, depth=0):
    """å•æ¬¡å—…æ¢é€»è¾‘ (æ”¯æŒ iframe ç©¿é€)"""
    if depth > 1: return None 

    try:
        headers = {
            'User-Agent': ua,
            'Referer': url,
            # å¢åŠ  Accept å¤´ï¼Œå‡è£…è‡ªå·±æ˜¯å¾ˆæ‡‚çš„æµè§ˆå™¨
            'Accept': 'text/html,application/xhtml+xml,application/json,text/javascript,*/*;q=0.01',
            'X-Requested-With': 'XMLHttpRequest' # å‡è£…æ˜¯ AJAX è¯·æ±‚ï¼Œè¯±éª—æœåŠ¡å™¨åå‡º JSON
        }
        
        response = requests.get(url, headers=headers, timeout=12, verify=False, allow_redirects=True)
        response.encoding = response.apparent_encoding 
        text = response.text
        
        # 1. ğŸ” æš´åŠ›æœç´¢å½“å‰é¡µé¢çš„æ‰€æœ‰è§’è½
        found_url = find_m3u8_deep(text)
        if found_url: return found_url

        # 2. ğŸ“¡ æ‰«æå†…åµŒçª—å£ (Iframe) -> é’»è¿›å»æ‰¾
        if depth == 0: # åªé’»ä¸€å±‚ï¼Œé˜²æ­¢æ­»å¾ªç¯
            iframe_pattern = r'<iframe[^>]+src=["\']([^"\']+)["\']'
            iframes = re.findall(iframe_pattern, text, re.I)
            
            for iframe_src in iframes:
                full_iframe_url = urllib.parse.urljoin(url, iframe_src)
                if 'ad' in full_iframe_url or 'google' in full_iframe_url: continue
                
                # é€’å½’è°ƒç”¨
                deep_found = sniff_single_ua(full_iframe_url, ua, depth + 1)
                if deep_found: return deep_found

    except Exception:
        pass
    return None

# --- æ ¸å¿ƒè§£ææ¨¡å— ---
def get_real_url(url, channel_name, retry_mode=False):
    is_yt = 'youtube.com' in url or 'youtu.be' in url
    
    # -------------------------------
    # ç­–ç•¥ A: æ²¹ç®¡ä¸“ç”¨ (yt-dlp)
    # -------------------------------
    if is_yt:
        cmd = ['yt-dlp', '-g', '--no-playlist', '--no-check-certificate', '--user-agent', UA_POOL["PC"]]
        cmd.extend(['-f', 'best[protocol^=m3u8]/best'])
        cmd.extend(['--referer', 'https://www.youtube.com/'])
        if os.path.exists(COOKIE_TEMP_FILE): cmd.extend(['--cookies', COOKIE_TEMP_FILE])
        cmd.append(url)
        
        try:
            res = subprocess.run(cmd, capture_output=True, text=True, timeout=40)
            if res.returncode == 0:
                raw = res.stdout.strip().split('\n')[0]
                if raw and 'http' in raw: return channel_name, raw, True
        except: pass
        return channel_name, None, False
    
    # -------------------------------
    # ç­–ç•¥ B: ç½‘ç«™è½®è¯¢å—…æ¢ (éæ²¹ç®¡)
    # -------------------------------
    else:
        # 1. å…ˆè¯•å®‰å“ (æ¦‚ç‡æœ€é«˜)
        url_android = sniff_single_ua(url, UA_POOL["Android"])
        if url_android: return channel_name, url_android, True
        
        # 2. å†è¯•ç”µè„‘ (æœ‰äº›è€ç½‘ç«™åªè®¤ç”µè„‘)
        url_pc = sniff_single_ua(url, UA_POOL["PC"])
        if url_pc: return channel_name, url_pc, True

        # 3. å®åœ¨ä¸è¡Œï¼Œç¥­å‡º yt-dlp è¯•è¯•è¿æ°”
        cmd = ['yt-dlp', '-g', '--no-playlist', '--no-check-certificate', '--user-agent', UA_POOL["Android"]]
        cmd.append(url)
        try:
            res = subprocess.run(cmd, capture_output=True, text=True, timeout=20)
            if res.returncode == 0:
                raw = res.stdout.strip().split('\n')[0]
                if raw and 'http' in raw: return channel_name, raw, True
        except: pass

    return channel_name, None, False

# --- ä¸»ç¨‹åºå…¥å£ ---
def update_streams():
    if not os.path.exists(JSON_FILE): return

    process_smart_cookies()
    
    try:
        with open(JSON_FILE, 'r', encoding='utf-8') as f: data = json.load(f)
    except Exception as e:
        print(f"âŒ JSON æ ¼å¼é”™è¯¯: {e}")
        return

    if "Run_Series_Loop" in data: data.pop("Run_Series_Loop") 
    
    stream_map = {}
    def extract(d):
        for k, v in d.items():
            if isinstance(v, dict): extract(v)
            elif isinstance(v, str) and v.startswith(('http', 'rtmp')): stream_map[k] = v
    extract(data)

    unique_tasks = {}
    for m in TARGET_FILES:
        if os.path.exists(m):
            with open(m, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.startswith('#EXTINF:'):
                        name = line.split(',')[-1].strip()
                        if name in stream_map: unique_tasks[name] = stream_map[name]

    live_tasks = [(k, v) for k, v in unique_tasks.items()]
    failed_channels = []
    
    print(f">>> [ä»»åŠ¡å°±ç»ª] ç›´æ’­é˜Ÿåˆ—: {len(live_tasks)}")

    # Phase 1: æ‰¹é‡å¹¶å‘
    if live_tasks:
        print(f"\n========================================")
        print(f"ğŸš€ [ç¬¬ä¸€é˜¶æ®µ] æ­£åœ¨æ›´æ–°ç›´æ’­é¢‘é“...")
        print(f"========================================")
        
        for i in range(0, len(live_tasks), BATCH_SIZE):
            batch = live_tasks[i:i+BATCH_SIZE]
            print(f"\nâš¡ [æ‰¹æ¬¡æ‰§è¡Œ] åºåˆ—: {i//BATCH_SIZE + 1}...")
            
            with ThreadPoolExecutor(max_workers=BATCH_SIZE) as executor:
                futures = {executor.submit(get_real_url, u, n, False): n for n, u in batch}
                for future in as_completed(futures):
                    n, u, success = future.result()
                    if success and u:
                        tag = "âœ… [è§£ææˆåŠŸ]"
                        if '.m3u8' in u and 'googlevideo' not in u: tag = "ğŸ” [ç½‘é¡µå—…æ¢]"
                        print(f"   {tag} {n}") 
                        unique_tasks[n] = u
                    else:
                        print(f"   ğŸŒªï¸ [æš‚ç¼“å¤„ç†] {n}")
                        orig = next((url for name, url in batch if name == n), None)
                        if orig: failed_channels.append((n, orig))
            time.sleep(0.5)

    # Phase 2: é‡è¯•
    if failed_channels:
        print(f"\n========================================")
        print(f"ğŸ”„ [æœ€ç»ˆæŒ½æ•‘] é›†ä¸­å¤„ç†æ‰€æœ‰å¼‚å¸¸ä»»åŠ¡...")
        print(f"========================================")
        
        for idx, (n, u) in enumerate(failed_channels):
            print(f"   ğŸ› ï¸ [æ­£åœ¨ä¿®å¤] {n} ...")
            retry_success = False
            # é‡è¯•åªè·‘ä¸€æ¬¡ï¼Œé¿å…æµªè´¹æ—¶é—´
            _, new_u, success = get_real_url(u, n, True)
            if success and new_u:
                print(f"      âœ… [å›æ»šæˆåŠŸ] é“¾è·¯å·²æ¢å¤")
                unique_tasks[n] = new_u
                retry_success = True
            
            if not retry_success: print(f"      âŒ [æœ€ç»ˆç†”æ–­] æ— æ³•æ¥é€šï¼Œå·²å¼ƒç”¨")

    # I/O å†™å…¥
    print("\n>>> [I/O æ“ä½œ] æ­£åœ¨å†™å…¥ç›®æ ‡æ–‡ä»¶...")
    for m in TARGET_FILES:
        if not os.path.exists(m): continue
        
        with open(m, 'r', encoding='utf-8') as f: lines = f.readlines()
        new_lines, idx, cnt = [], 0, 0
        while idx < len(lines):
            line = lines[idx]
            if line.startswith('#EXTINF:'):
                name = line.split(',')[-1].strip()
                if name in unique_tasks:
                    new_lines.append(line)
                    new_lines.append(unique_tasks[name] + '\n')
                    idx += 2; cnt += 1; continue
            new_lines.append(line); idx += 1
        with open(m, 'w', encoding='utf-8') as f: f.writelines(new_lines)
        print(f"   -> {m}: æ›´æ–°è®°å½• {cnt} æ¡")
    
    print("\nâœ… [æ‰§è¡Œå®Œæ¯•] æ‰€æœ‰è®¡åˆ’ä»»åŠ¡å·²å®Œæˆã€‚")

if __name__ == '__main__':
    update_streams()
